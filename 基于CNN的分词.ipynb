{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  语料处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86924it [00:19, 4398.16it/s]\n"
     ]
    }
   ],
   "source": [
    "# 没有找到人民日报2014语料\n",
    "# 使用msr语料\n",
    "\n",
    "# 使用bmes标记\n",
    "\n",
    "char_counter = Counter()\n",
    "texts = []\n",
    "tags = []\n",
    "\n",
    "stops = u'，。！？；、：,\\.!\\?;:\\n'\n",
    "for line in tqdm(open(\"/home/xueyou/tmp/icwb2-data/training/msr_training.utf8\")):\n",
    "    txt = [i.strip() for i in re.split('['+stops+']',line) if i.strip()]\n",
    "    for t in txt:\n",
    "        texts.append(\"\")\n",
    "        tags.append('')\n",
    "        for w in re.split(\" +\",t):\n",
    "            texts[-1] += w\n",
    "            char_counter.update(list(w))\n",
    "            if len(w) == 1:\n",
    "                tags[-1] += 's'\n",
    "            else:\n",
    "                tags[-1] += 'b' + 'm'*(len(w)-2) + 'e'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'响起在农村大地上的钟声－－－看电视纪录片《村民的选择》'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sssbebessbessssbebmesbesbes'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "314148"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2id = {'<unk>':0,'<pad>':1}\n",
    "min_count = 2\n",
    "for w,cnt in char_counter.most_common():\n",
    "    if cnt >= min_count:\n",
    "        word2id[w] = len(word2id)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4728"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_word2id = {\"b\":0,\"m\":1,\"e\":2,\"s\":3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=314148, minmax=(1, 581), mean=11.908431694615278, variance=93.847848128829185, skewness=20.03770976920095, kurtosis=1081.4968389673104)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "stats.describe([len(s) for s in texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def pad(s,max_len,pad_char):\n",
    "        s = s[:max_len]\n",
    "        s = s + [pad_char] * (max_len-len(s))\n",
    "        return s\n",
    "    \n",
    "def data_iter(batch_size,max_len):\n",
    "    global texts,tags\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    ltexts,ltags = shuffle(texts,tags)\n",
    "    \n",
    "    max_batch = -1\n",
    "    for text,tag in zip(ltexts,ltags):\n",
    "        if len(x) == batch_size:\n",
    "            \n",
    "            max_len = min(max_len,max_batch)\n",
    "            max_batch = -1\n",
    "            tmp_x,tmp_y=[],[]\n",
    "            for text,tag in zip(x,y):\n",
    "                text = pad(text,max_len,1)\n",
    "                tag = pad(tag,max_len,3)\n",
    "                tmp_x.append(text)\n",
    "                tmp_y.append(tag)\n",
    "            yield tmp_x,tmp_y\n",
    "            \n",
    "            x=[]\n",
    "            y=[]\n",
    "\n",
    "        text = [word2id.get(t,0) for t in text]\n",
    "        tag = [target_word2id.get(t,3) for t in tag]\n",
    "        max_batch = max(max_batch,len(text))\n",
    "            \n",
    "        x.append(text)\n",
    "        y.append(tag)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义和训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.0'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Config():\n",
    "    embedding_size = 128\n",
    "    keep_prob = 0.5\n",
    "    max_len = 80\n",
    "    batch_size = 1024\n",
    "    src_vocab_size = len(word2id)\n",
    "    tgt_vocab_size = len(target_word2id)\n",
    "    train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CNN_Model(object):\n",
    "    def __init__(self,config):\n",
    "        self.config = config\n",
    "        self.build_model()\n",
    "    \n",
    "    def build_model(self):\n",
    "        embedding_size = self.config.embedding_size\n",
    "        src_vocab_size = self.config.src_vocab_size\n",
    "        tgt_vocab_size = self.config.tgt_vocab_size\n",
    "        train = self.config.train\n",
    "        \n",
    "        keep_prob_p = tf.placeholder(tf.float32)\n",
    "        x = tf.placeholder(tf.int32,[None,None])\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        input_embedding = tf.Variable(tf.random_uniform([src_vocab_size,embedding_size],-1,1),name=\"embedding\")\n",
    "        inputs_e = tf.nn.embedding_lookup(input_embedding,x)\n",
    "        inputs_drop_out = tf.nn.dropout(inputs_e,keep_prob_p)\n",
    "\n",
    "        conv1 = tf.layers.conv1d(inputs_drop_out,filters=embedding_size,kernel_size=3,padding=\"SAME\",activation=tf.nn.relu)\n",
    "        conv1 = tf.nn.dropout(conv1,keep_prob_p)\n",
    "        conv2 = tf.layers.conv1d(conv1,filters=embedding_size/2,kernel_size=3,padding=\"SAME\",activation=tf.nn.relu)\n",
    "        conv2 = tf.nn.dropout(conv2,keep_prob_p)\n",
    "        conv3 = tf.layers.conv1d(conv1,filters=tgt_vocab_size,kernel_size=3,padding=\"SAME\")\n",
    "\n",
    "        self.x = x\n",
    "        self.logits = tf.nn.softmax(conv3)\n",
    "        self.keep_prob = keep_prob_p\n",
    "        \n",
    "        if train:\n",
    "            y = tf.placeholder(tf.int32, shape=[None,None])\n",
    "            crossent = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=conv3)\n",
    "            losses = tf.reduce_sum(\n",
    "                crossent) / tf.to_float(batch_size)\n",
    "            self.train_step = tf.train.AdamOptimizer().minimize(losses)\n",
    "            correct_prediction = tf.equal(tf.argmax(conv3, 2), tf.cast(y,dtype=tf.int64))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            self.y = y\n",
    "            self.losses = losses\n",
    "    \n",
    "    def train_one_step(self,sess,x,y,keep_prob):\n",
    "        _,loss = sess.run([self.train_step,self.losses],feed_dict={self.x:x,self.y:y,self.keep_prob:keep_prob})\n",
    "        return loss\n",
    "        \n",
    "    def get_accuracy(self,sess,x,y):\n",
    "        acc = sess.run(self.accuracy,feed_dict={self.x:x,self.y:y,self.keep_prob:1.0})\n",
    "        return acc\n",
    "    \n",
    "    def predict(self,sess,x):\n",
    "        logits = sess.run(self.logits,feed_dict={self.x:x,self.keep_prob:1.0})\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epcho 1, Accuracy: 0.948003, Loss: 8.67332, batches: 305/306\n",
      "Epcho 1 Mean Accuracy: 0.920798, Mean Loss: 12.5983\n",
      "Epcho 2, Accuracy: 0.955172, Loss: 7.77253, batches: 305/306\n",
      "Epcho 2 Mean Accuracy: 0.954257, Mean Loss: 8.04857\n",
      "Epcho 3, Accuracy: 0.973523, Loss: 7.03106, batches: 305/306\n",
      "Epcho 3 Mean Accuracy: 0.973355, Mean Loss: 7.11379\n",
      "Epcho 4, Accuracy: 0.965275, Loss: 6.26814, batches: 305/306\n",
      "Epcho 4 Mean Accuracy: 0.962058, Mean Loss: 6.59512\n",
      "Epcho 5, Accuracy: 0.95981, Loss: 5.91445, batches: 305/306\n",
      "Epcho 5 Mean Accuracy: 0.95878, Mean Loss: 6.22576\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    config = Config()\n",
    "    model = CNN_Model(config)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    batches = int(len(texts)/config.batch_size)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    for i in range(5):\n",
    "        accs = []\n",
    "        losses = []\n",
    "        cnt = 0\n",
    "        for xx,yy in data_iter(config.batch_size,config.max_len):\n",
    "            xx,yy = np.asarray(xx),np.asarray(yy)\n",
    "            acc = model.get_accuracy(sess,xx,yy)\n",
    "            accs.append(acc)\n",
    "            loss = model.train_one_step(sess,xx,yy,config.keep_prob)\n",
    "            losses.append(loss)\n",
    "            print('\\rEpcho %s, Accuracy: %s, Loss: %s, batches: %s/%s'%(i+1, acc, loss, cnt, batches),end='')\n",
    "            cnt += 1\n",
    "\n",
    "        print(\"\")\n",
    "        print('Epcho %s Mean Accuracy: %s, Mean Loss: %s'%(i+1, np.mean(accs),np.mean(losses)))\n",
    "        saver.save(sess, '/home/xueyou/tmp/cnn_cuts/cnn_cut.ckpt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states = ['b','m','e','s']\n",
    "\n",
    "# use trans prob from jieba\n",
    "trans_proba={'b': {'e': -0.510825623765990, 'm': -0.916290731874155},\n",
    "             'e': {'b': -0.5897149736854513, 's': -0.8085250474669937},\n",
    "             'm': {'e': -0.33344856811948514, 'm': -1.2603623820268226},\n",
    "             's': {'b': -0.7211965654669841, 's': -0.6658631448798212}}\n",
    "\n",
    "start_proba={'b': -0.26268660809250016,\n",
    "             'e': -3.14e+100,\n",
    "             'm': -3.14e+100,\n",
    "             's': -1.4652633398537678}\n",
    "\n",
    "PrevStatus = {\n",
    "    'b': 'es',\n",
    "    'm': 'mb',\n",
    "    's': 'se',\n",
    "    'e': 'bm'\n",
    "}\n",
    "\n",
    "def veterbi(probs):\n",
    "    V = [{}]\n",
    "    path = {}\n",
    "    for i,y in enumerate(states):\n",
    "        V[0][y] = np.log(probs[0][i]) + start_proba[y]\n",
    "        path[y] = [y]\n",
    "    for t in range(1,len(probs)):\n",
    "        V.append({})\n",
    "        new_path = {}\n",
    "        for j,y in enumerate(states):\n",
    "            emit = np.log(probs[t][j])\n",
    "            prob,state = max((V[t-1][y0] + trans_proba[y0][y] + emit,y0)  for y0 in PrevStatus[y])\n",
    "            V[t][y] = prob\n",
    "            new_path[y] = path[state] + [y]\n",
    "        path = new_path\n",
    "    prob, state = max((V[len(probs) - 1][y], y) for y in 'es')\n",
    "    return prob,path[state]\n",
    "   \n",
    "def cut(sentence,path):\n",
    "    words = [sentence[0]]\n",
    "    for i in range(1,len(sentence)):\n",
    "        if path[i] in ['s','b']:\n",
    "            words.append(sentence[i])\n",
    "        else:\n",
    "            words[-1] += sentence[i]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/xueyou/tmp/cnn_cuts/cnn_cut.ckpt\n",
      "我 今天 不 开心 。\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    config = Config()\n",
    "    config.train=False\n",
    "    model = CNN_Model(config)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('/home/xueyou/tmp/cnn_cuts/'))\n",
    "    \n",
    "    raw_text = '我今天不开心。'\n",
    "    text = [word2id.get(t,0) for t in raw_text]\n",
    "    text = pad(text,len(text),1)\n",
    "    x = np.reshape(np.asarray(text),[1,-1])\n",
    "    p = model.predict(sess,x)[0]\n",
    "    _,path = veterbi(p)\n",
    "    words = cut(raw_text,path)\n",
    "    print(\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/xueyou/tmp/cnn_cuts/cnn_cut.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    config = Config()\n",
    "    config.train=False\n",
    "    model = CNN_Model(config)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('/home/xueyou/tmp/cnn_cuts/'))\n",
    "    \n",
    "    test_texts = open(\"/home/xueyou/tmp/icwb2-data/testing/pku_test.utf8\").read().split(\"\\n\")[:-1]\n",
    "    with open(\"/home/xueyou/tmp/icwb2-data/testing/pku_cnn_cut.txt\",'w') as f:    \n",
    "        for raw_text in test_texts:\n",
    "            if raw_text:\n",
    "                text = [word2id.get(t,0) for t in raw_text]\n",
    "                text = pad(text,len(text),1)\n",
    "                x = np.reshape(np.asarray(text),[1,-1])\n",
    "                p = model.predict(sess,x)[0]\n",
    "                _,path = veterbi(p)\n",
    "                words = cut(raw_text,path)\n",
    "                f.write(\" \".join(words) + \"\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析和思考\n",
    "- 利用score代码测试准确率，只有82%，recall是83%\n",
    "    - 现在这个模型没有经过充分的优化，不知道原作者为什么可以有93%的准确率\n",
    "    - 我通过修改模型结构，如增加kernel size，提高了准确率。这个模型本身是不复杂的，因此可以提高模型复杂度来提高精度\n",
    "    - 可以利用S2S+Attention模型来做这个事情，效果肯定会更好\n",
    "- 文章还提到硬解码的问题，思路比较简单，只是在预测出来的标签概率上乘以对应的放大数值，以便在解码时提高单词的概率"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [jason_py3]",
   "language": "python",
   "name": "Python [jason_py3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
