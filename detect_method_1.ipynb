{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要参考了两篇博客：\n",
    "- http://www.matrix67.com/blog/archives/5044\n",
    "- http://spaces.ac.cn/archives/3491/\n",
    "\n",
    "主要的思想是利用统计信息进行过滤，用到的统计信息包括：\n",
    "- 词频\n",
    "- 单词内部的凝合程度\n",
    "- 单词上下文的信息熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jason\\Downloads\n"
     ]
    }
   ],
   "source": [
    "cd ../Downloads/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = open(\"./天龙八部精校版.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1258690"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_set = set([u'】',u'【', u'，', u'\\n', u'。', u'、', u'：', u'(', u')', u'[', u']', u'.', u',', u' ', u'\\u3000', u'”', u'“', u'？', u'?', u'！', u'‘', u'’', u'…'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_count = 10 # 最小频数\n",
    "min_support = 30 # 最小的凝合程度\n",
    "min_ms = 3 # 最小的信息熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 最好是按照split set把文本分割成短句子，然后在短句子基础上再做新词发现\n",
    "# 可能效果会更好，但是在大文本的情况下，个人觉得直接去除标点字符也会取得不错的效果\n",
    "for i in split_set:\n",
    "    data=data.replace(i,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gram_1 = Counter(list(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gram_1 = dict(gram_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grams = [gram_1]\n",
    "max_gram = 4\n",
    "ngram_pattern = {2:'(..)', 3:'(...)', 4:'(....)', 5:'(.....)', 6:'(......)', 7:'(.......)'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remains = [set(list(gram_1.keys()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genetate 2-gram\n",
      "raw 2-gram count 218814\n",
      "after filter: 14554\n",
      "2-gram candidates: 3611\n",
      "genetate 3-gram\n",
      "raw 3-gram count 624263\n",
      "after filter: 6632\n",
      "3-gram candidates: 5563\n",
      "genetate 4-gram\n",
      "raw 4-gram count 865724\n",
      "after filter: 1879\n",
      "4-gram candidates: 1869\n"
     ]
    }
   ],
   "source": [
    "for n in range(2,max_gram+1):\n",
    "    print(\"genetate {0}-gram\".format(n))\n",
    "    all_xgram = []\n",
    "    for i in range(n):\n",
    "        all_xgram = all_xgram + re.findall(ngram_pattern[n],data[i:])\n",
    "    gram_x = dict(Counter(all_xgram))\n",
    "    grams.append(gram_x)\n",
    "    print(\"raw {0}-gram count {1}\".format(n,len(gram_x)))\n",
    "    gram_x = {k:v for k,v in gram_x.items() if v > min_count}\n",
    "    print(\"after filter: {0}\".format(len(gram_x)))\n",
    "    \n",
    "    #凝合程度\n",
    "    remain = []\n",
    "    for word in gram_x:\n",
    "        for k in range(1,n):\n",
    "            left = word[:k]\n",
    "            left_count = grams[len(left)-1][left]\n",
    "            right = word[k:]\n",
    "            right_count = grams[len(right)-1][right]\n",
    "            a = total * gram_x[word] / left_count / right_count\n",
    "            if a >= min_support:\n",
    "                remain.append(word)\n",
    "    remains.append(set(remain))\n",
    "    print(\"{0}-gram candidates: {1}\".format(n,len(remains[-1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def info_entropy(array):\n",
    "    array = np.asarray(array)\n",
    "    return np.sum(-(array / array.sum() ) * (np.log(array / array.sum())))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get context info\n",
      "filter based on entropy\n",
      "2-gram words: 244\n",
      "get context info\n",
      "filter based on entropy\n",
      "3-gram words: 188\n",
      "get context info\n",
      "filter based on entropy\n",
      "4-gram words: 46\n"
     ]
    }
   ],
   "source": [
    "final_words = []\n",
    "for n in range(2,max_gram+1):\n",
    "    context=[]\n",
    "    for i in range(n):\n",
    "        context = context + re.findall('(.){0}(.)'.format(ngram_pattern[n]),data[i:])\n",
    "    mutual_info = {}\n",
    "    remain_word = remains[n-1]\n",
    "    print(\"get context info\")\n",
    "    for left,word,right in context:\n",
    "        if word not in remain_word:\n",
    "            continue\n",
    "        if word not in mutual_info:\n",
    "            mutual_info[word] = {'left':{},'right':{}}\n",
    "        if left not in mutual_info[word]['left']:\n",
    "            mutual_info[word]['left'][left] = 0\n",
    "        if right not in mutual_info[word]['right']:\n",
    "            mutual_info[word]['right'][right] = 0\n",
    "        mutual_info[word]['left'][left] += 1\n",
    "        mutual_info[word]['right'][right] += 1\n",
    "    print(\"filter based on entropy\")\n",
    "    \n",
    "    words = []\n",
    "    for word in remain_word:\n",
    "        if word not in mutual_info:\n",
    "            continue\n",
    "        left = np.array(list(mutual_info[word]['left'].values()))\n",
    "        right = np.array(list(mutual_info[word]['right'].values()))\n",
    "        ls = info_entropy(left)\n",
    "        rs = info_entropy(right)\n",
    "        ms = min(ls,rs)\n",
    "        if ms >= min_ms:\n",
    "            words.append(word)\n",
    "    print(\"{0}-gram words: {1}\".format(n,len(words)))\n",
    "    final_words = final_words + words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = [(w,grams[len(w)-1][w]) for w in final_words]\n",
    "words = sorted(words,key=lambda w: w[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "段誉 3375\n",
      "萧峰 1787\n",
      "虚竹 1638\n",
      "自己 1638\n",
      "什么 1595\n",
      "阿紫 1151\n",
      "乔峰 1131\n",
      "武功 1073\n",
      "甚么 1034\n",
      "阿朱 986\n",
      "姑娘 983\n",
      "慕容复 927\n",
      "王语嫣 859\n",
      "师父 818\n",
      "咱们 818\n",
      "如何 777\n",
      "段正淳 758\n",
      "木婉清 735\n",
      "如此 699\n",
      "突然 654\n",
      "心想 606\n",
      "大理 598\n",
      "帮主 592\n",
      "童姥 583\n",
      "弟子 580\n",
      "不敢 578\n",
      "鸠摩智 564\n",
      "丐帮 562\n",
      "怎么 541\n",
      "游坦之 534\n",
      "众人 526\n",
      "内力 509\n",
      "倘若 505\n",
      "南海鳄神 485\n",
      "之间 474\n",
      "脸上 470\n",
      "段誉道 468\n",
      "兄弟 467\n",
      "原来 464\n",
      "这些 455\n",
      "钟灵 442\n",
      "丁春秋 437\n",
      "天下 434\n",
      "跟着 422\n",
      "当真 415\n",
      "说话 415\n",
      "爹爹 412\n",
      "包不同 410\n",
      "伸手 405\n",
      "登时 402\n",
      "功夫 400\n",
      "和尚 397\n",
      "今日 393\n",
      "少林寺 388\n",
      "没有 372\n",
      "保定帝 341\n",
      "眼见 334\n",
      "左手 333\n",
      "当即 331\n",
      "英雄 327\n",
      "师兄 327\n",
      "有什么 327\n",
      "虽然 324\n",
      "双手 319\n",
      "阿碧 307\n",
      "这般 306\n",
      "段延庆 302\n",
      "乌老大 302\n",
      "厉害 301\n",
      "马夫人 300\n",
      "之极 294\n",
      "只怕 292\n",
      "右手 282\n",
      "十分 277\n",
      "便即 277\n",
      "不由得 277\n",
      "哪里 276\n",
      "一阵 272\n",
      "只听得 271\n",
      "玄难 266\n",
      "似乎 264\n",
      "不肯 264\n",
      "方丈 264\n",
      "对方 260\n",
      "胸口 253\n",
      "居然 251\n",
      "明白 251\n",
      "中原 249\n",
      "小僧 247\n",
      "虚竹道 245\n",
      "轻轻 240\n",
      "无法 240\n",
      "了出来 240\n",
      "玄慈 239\n",
      "仍是 238\n",
      "此刻 238\n",
      "王夫人 238\n",
      "※※※ 236\n",
      "那少女 234\n",
      "只觉 232\n"
     ]
    }
   ],
   "source": [
    "for word,count in words[:100]:\n",
    "    print(word,count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 思考\n",
    "可以发现这个方法可以找到诸如人名等一些新词，但是还是存在一些问题，比如像“段誉道”，“了出来”这样一些bad cases。自己想了一些解决方法：\n",
    "- 通过对不同n-gram设置不同的threshold\n",
    "- 仅仅通过这3个feature是不够的，看论文的话一般还会利用语法的一些feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
